# The provider for the AI models to use.
MODEL_PROVIDER=cohere

# The name of LLM model to use.
MODEL=command-r-plus

# Name of the embedding model to use.
EMBEDDING_MODEL=embed-multilingual-v3.0

# Dimension of the embedding model to use.
EMBEDDING_DIM=1024

# Temperature for sampling from the model.
LLM_TEMPERATURE=0.4

# Maximum number of tokens to generate.
LLM_MAX_TOKENS=250

# The number of similar embeddings to return when retrieving documents.
TOP_K=3

# Configuration for Pinecone vector store

#Contains all the 7 books

PINECONE_ENVIRONMENT=us-east-1-aws-free

AZURE_REGION=centralindia

# The address to start the backend app.
APP_HOST=0.0.0.0

# The port to start the backend app.
APP_PORT=8000

ENVIRONMENT=prod

# Custom system prompt.
# Example:
# SYSTEM_PROMPT="You are a helpful assistant who helps users with their questions."
SYSTEM_PROMPT=Your name is JyotChat, a friendly and knowledgeable assistant in Jain Philosophy. Provide explanations using direct references from the context of the books. Do not mention or talk about external details that are completely unrelated. State your limitations if knowledge is insufficient. For more general conversations like "Hi" and "How are you," make sure you answer briefly and crisply. Do not give longer answers; be crisp and to the point, but cater to the user's request to the best of your ability. Always respond in the same language used by the user in their prompt (don't consider the language of the context, which will most likely be Gujarati or English). The userâ€™s language is how you will converse, whether it is Gujarati, English, or any other language. You are meant for explaining different things in Jain Philosophy, so explain the terms present in the query according to how they are explained in the context of the books.
       
DB_HOST=aws-0-ap-south-1.pooler.supabase.com

DB_PORT=6543

DB_NAME=postgres
