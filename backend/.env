# The Llama Cloud API key.
# LLAMA_CLOUD_API_KEY=

# The provider for the AI models to use.
MODEL_PROVIDER=cohere

# The name of LLM model to use.
MODEL=command-r-plus

# Name of the embedding model to use.
EMBEDDING_MODEL=embed-multilingual-v3.0

# Dimension of the embedding model to use.
EMBEDDING_DIM=1024

# The Cohere API key to use.
COHERE_API_KEY=kSwaZZMOtpcxHcjm8wUXEjP5eSevho5QM74zd3qT
# Temperature for sampling from the model.
LLM_TEMPERATURE=0.6

# Maximum number of tokens to generate.
LLM_MAX_TOKENS=500

# The number of similar embeddings to return when retrieving documents.
TOP_K=3

# Configuration for Pinecone vector store
# The Pinecone API key.
PINECONE_API_KEY=83c5e56e-aaa2-4d6d-ac69-ca04e632d8c3

PINECONE_ENVIRONMENT=us-east-1-aws-free

PINECONE_INDEX_NAME=jyot-books-pdf

# The address to start the backend app.
APP_HOST=0.0.0.0

# The port to start the backend app.
APP_PORT=8000

# Custom system prompt.
# Example:
# SYSTEM_PROMPT="
# We have provided context information below.
# ---------------------
# {context_str}
# ---------------------
# Given this information, please answer the question: {query_str}
# "

SYSTEM_PROMPT="
As an interactive assistant, your main task is to respond to various questions and requests about the teachings of Jyot and its Guru, Acharya Yugbhushan Suriji. 
You will use a comprehensive context to research and deliver the most suitable answers, with a focus on addressing the user's needs.
---------------------
{context_str}
---------------------
Always respond in the same language as the user's current question. Ensure your responses are composed of complete sentences, with correct grammar and spelling. 
The context documents will be in Gujarati; use these as a reference to answer the user's query in their preferred language. If the assistant encounters a request 
that is unrelated to the teachings of Jyot and its Guru, Acharya Yugbhushan Suriji, or if there is not enough context to provide a meaningful response, the assistant 
should politely express its limitations and encourage the user to ask questions that are more relevant to the topic at hand.
---------------------
Given this information, please answer the question: {query_str}.
"
