# The provider for the AI models to use.
MODEL_PROVIDER=cohere

# The name of LLM model to use.
MODEL=command-r-plus

# Name of the embedding model to use.
EMBEDDING_MODEL=embed-multilingual-v3.0

# Dimension of the embedding model to use.
EMBEDDING_DIM=1024

# Temperature for sampling from the model.
LLM_TEMPERATURE=0.4

# Maximum number of tokens to generate.
LLM_MAX_TOKENS=250

# The number of similar embeddings to return when retrieving documents.
TOP_K=3

# Configuration for Pinecone vector store

#Contains all the 7 books

PINECONE_ENVIRONMENT=us-east-1-aws-free

AZURE_REGION=centralindia

# The address to start the backend app.
APP_HOST=0.0.0.0

# The port to start the backend app.
APP_PORT=8000

ENVIRONMENT=prod

# Custom system prompt.
# Example:
# SYSTEM_PROMPT="You are a helpful assistant who helps users with their questions."
SYSTEM_PROMPT = As a friendly and knowledgeable Assistant in Jain Philosophy, provide explanations using direct references from the context of the books. Do not mention book names or external details. Always respond in the same language used by the user in his prompt (dont consider the language of the context that will most likely be gujarati or english) but the user language is how you will converse, whether it is Gujarati, English, or any any other language. You are meant for explaining different things in Jain Philosophy so explain the terms present in the query according to how its explained inside the context.
       
DB_HOST=aws-0-ap-south-1.pooler.supabase.com

DB_PORT=6543

DB_NAME=postgres
